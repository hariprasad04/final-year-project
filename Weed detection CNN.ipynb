{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kaROzVQt00A"
   },
   "source": [
    "# Dataset \n",
    "Dataset used: https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops\n",
    "\n",
    "**15336 Total: 9202 train, 3067 valid, 3067 test**\n",
    "*   3249 soil: 1949 train, 650 valid, 650 test\n",
    "*   7376 soybean:4426 train, 1475 valid, 1475 test\n",
    "*   3520 grass: 2112 train, 704 valid, 704 test\n",
    "*  1191 weed: 715 train, 238 valid,238 test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip weed-detection-in-soybean-crops.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JTc5Jm958tW"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f042b0da96b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# El directorio donde guardaremos el dataset se lllama \"weedandcrops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/Project 2/dataset'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Directorio para nuestro train, validation y test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Creaci칩n de directorios\n",
    "\n",
    "#Creaci칩n de directorios\n",
    "\n",
    "# El directorio donde guardaremos el dataset se lllama \"weedandcrops\"\n",
    "\n",
    "base_dir = 'D:/Project 2/dataset'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Directorio para nuestro train, validation y test\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#Directorio para training de grass,soil,soybean, weed\n",
    "train_grass_dir = os.path.join(train_dir, 'grass')\n",
    "os.makedirs(train_grass_dir, exist_ok=True)\n",
    "\n",
    "train_soil_dir = os.path.join(train_dir, 'soil')\n",
    "os.makedirs(train_soil_dir, exist_ok=True)\n",
    "\n",
    "train_soybean_dir = os.path.join(train_dir, 'soybean')\n",
    "os.makedirs(train_soybean_dir, exist_ok=True)\n",
    "\n",
    "train_weed_dir = os.path.join(train_dir, 'weed')\n",
    "os.makedirs(train_weed_dir, exist_ok=True)\n",
    "\n",
    "#Directorio para validation de grass,soil,soybean, weed\n",
    "validation_grass_dir = os.path.join(validation_dir, 'grass')\n",
    "os.makedirs(validation_grass_dir, exist_ok=True)\n",
    "\n",
    "validation_soil_dir = os.path.join(validation_dir, 'soil')\n",
    "os.makedirs(validation_soil_dir, exist_ok=True)\n",
    "\n",
    "validation_soybean_dir = os.path.join(validation_dir, 'soybean')\n",
    "os.makedirs(validation_soybean_dir, exist_ok=True)\n",
    "\n",
    "validation_weed_dir = os.path.join(validation_dir, 'weed')\n",
    "os.makedirs(validation_weed_dir, exist_ok=True)\n",
    "\n",
    "#Directorio para test de grass,soil,soybean, weed\n",
    "test_grass_dir = os.path.join(test_dir, 'grass')\n",
    "os.makedirs(test_grass_dir, exist_ok=True)\n",
    "\n",
    "test_soil_dir = os.path.join(test_dir, 'soil')\n",
    "os.makedirs(test_soil_dir, exist_ok=True)\n",
    "\n",
    "test_soybean_dir = os.path.join(test_dir, 'soybean')\n",
    "os.makedirs(test_soybean_dir, exist_ok=True)\n",
    "\n",
    "test_weed_dir = os.path.join(test_dir, 'weed')\n",
    "os.makedirs(test_weed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWqGRcn8Cjz7"
   },
   "outputs": [],
   "source": [
    "#Copiamos las im치genes a los directorios correspondientes\n",
    "\n",
    "#This image dataset has 15336 segments, being 3249 of soil, 7376 of soybean, 3520 grass and 1191 of broadleaf weeds.\n",
    "#60%train, 20% validation, 20%test\n",
    "\n",
    "#3249 soil: 1949 train, 650 valid, 650 test\n",
    "#7376 soybean:4426 train, 1475 valid, 1475 test\n",
    "#3520 grass: 2112 train, 704 valid, 704 test\n",
    "#1191 weed: 715 train, 238 valid,238 test\n",
    "\n",
    "#15336 Total: 9202 train, 3067 valid, 3067 test\n",
    "\n",
    "import shutil\n",
    "\n",
    "#File original del que vamos a copiar las im치genes\n",
    "original_dataset_dir_soil = 'D:/Project 2/dataset/soil'\n",
    "original_dataset_dir_soybean = 'D:/Project 2/dataset/soybean'\n",
    "original_dataset_dir_grass = 'D:/Project 2/dataset/grass'\n",
    "original_dataset_dir_weed = 'D:/Project 2/dataset/broadleaf'\n",
    "\n",
    "####SOIL####\n",
    "# Copiamos las primeras 1949 imagenes de soil a train_soil_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(1,1950)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soil, fname)\n",
    "    dst = os.path.join(train_soil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Las siguientes 650 a validation_soil_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(1950, 2600)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soil, fname)\n",
    "    dst = os.path.join(validation_soil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copiamos las siguientes 650 a test_soil_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(2600, 3250)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soil, fname)\n",
    "    dst = os.path.join(test_soil_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "####SOYBEAN####\n",
    "# Copiamos las primeras 4426 imagenes de soybean a train_soybean_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(1,4427)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soybean, fname)\n",
    "    dst = os.path.join(train_soybean_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Las siguientes 1475 a validation_soybean_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(4427, 5902)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soybean, fname)\n",
    "    dst = os.path.join(validation_soybean_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copiamos las siguientes 1475 a test_soybean_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(5902, 7377)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_soybean, fname)\n",
    "    dst = os.path.join(test_soybean_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "####GRASS####\n",
    "# Copiamos las primeras 2112 imagenes de grass a train_grass_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(1,2113)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_grass, fname)\n",
    "    dst = os.path.join(train_grass_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Las siguientes 704 a validation_grass_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(2113, 2817)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_grass, fname)\n",
    "    dst = os.path.join(validation_grass_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copiamos las siguientes 704 a test_grass_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(2817, 3521)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_grass, fname)\n",
    "    dst = os.path.join(test_grass_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "####WEED####\n",
    "# Copiamos las primeras 715 imagenes de weed a train_weed_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(1,716)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_weed, fname)\n",
    "    dst = os.path.join(train_weed_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Las siguientes 238 a validation_weed_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(716, 954)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_weed, fname)\n",
    "    dst = os.path.join(validation_weed_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copiamos las siguientes 238 a test_weed_dir\n",
    "fnames = ['{}.tif'.format(i) for i in range(954, 1192)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir_weed, fname)\n",
    "    dst = os.path.join(test_weed_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mgp4IhN-jw91"
   },
   "source": [
    "# Redes convolucionales\n",
    "\n",
    "M칠todos de regularizaci칩n: Data augmentation y Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGlOIfo4VxXh"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nhvP4wH8jrnU",
    "outputId": "111973b1-b05e-4a38-fe44-54d50e870c02"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "ctUufrYMj42D",
    "outputId": "645a67f7-bfd7-4e88-d577-0cfbbd0692a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 3,454,660\n",
      "Trainable params: 3,454,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definir el modelo\n",
    "#Droput se usa para generalizar y evitar el overfit\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150,150,3))) #(3 canales iniciales*32 nodos*3pix*3pix)+32 bias= 896 param\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) #(32 nodos posteriores*64 nodos*3pix*3pix)+64 bias= 18496 param\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) #(64 nodos posteriores*128 nodos*3pix*3pix)+128 bias= 73856 param\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) #(128 nodos posteriores*128 nodos*3pix*3pix)+128 bias= 147584 param\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten()) #7*7*128 del max pooling anterior\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu')) #(6272 de la capa anterior *512 capa densa)+512 bias = 3211776\n",
    "model.add(layers.Dense(4, activation='softmax')) #(512 capa anterior*4 capa densa)+4 bias = 2052\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uh4g8AdupoHx"
   },
   "outputs": [],
   "source": [
    "#Definir funci칩n de p칠rdida y optimizador\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMo516fdWAOw"
   },
   "source": [
    "## Data augmentation and re-escaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xTAfGFLZWtbt",
    "outputId": "a3ee6fc8-d84e-4e2b-800e-abfb9fde20b8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8a787cb2555e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m train_generator = train_datagen.flow_from_directory(\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Directorio de train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Reescalamos todas las imagenes a 150x150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Este es un modulo con utilidades para preprocesamiento de imagenes\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#ImageDataGenerator generates batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).\n",
    "#Rather than performing the operations on your memory, the API is designed to be iterated by the deep learning model fitting process, \n",
    "#creating augmented image data for you just-in-time. \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#>>>>Instanciar ImageDataGenerator para aumentar data de train /255\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "#>>>>Notemos que la data de validacion no debe ser aumentada. Solo /255\n",
    "\n",
    "#>>>>Instanciar ImageDataGenerator para /255 la data de test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "####Generar nueva data de train y re-escalar####\n",
    "#Flow_from_directory: Is useful when the images are placed in there respective label folders. This method will identify classes automatically from the folder name.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Directorio de train\n",
    "        train_dir,\n",
    "        # Reescalamos todas las imagenes a 150x150\n",
    "        target_size=(150, 150),\n",
    "        color_mode=\"rgb\",\n",
    "        #batch_size: No. of images to be yielded from the generator per batch.\n",
    "        #The number of images within each folder is not required to be divisible by the batch size. \n",
    "        #If there were a remainder, if you had 234 images when using a batch size of 10, then the last batch would just be smaller. \n",
    "        #It would be a batch of 4 in this case.\n",
    "        batch_size=92,\n",
    "        # Ya que usamos como p칠rdida categorical_crossentropy loss, necesitamos etiquetas tipo categorical.\n",
    "        class_mode='categorical')\n",
    "\n",
    "####Re-escalar data de validation####\n",
    "#Flow_from_directory: Is useful when the images are placed in there respective label folders. This method will identify classes automatically from the folder name.\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=31,\n",
    "        class_mode='categorical')\n",
    "\n",
    "####Re-escalar data de test####\n",
    "#Flow_from_directory: Is useful when the images are placed in there respective label folders. This method will identify classes automatically from the folder name.\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=31,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9LPWZ0OvqqT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "9Bk2qk32vqBc",
    "outputId": "d1d7c0ba-bd9c-48c9-cd22-1bdf8d5f1821"
   },
   "outputs": [],
   "source": [
    "#Para definir steps_per_epoch\n",
    "import math\n",
    "training_samples =9202\n",
    "batch_size_training_generator=92\n",
    "validation_samples =3067\n",
    "batch_size_validation_generator=31\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      ##In each epoch, the ImageDataGenerator applies a transformation on the images you have and use the transformed images for training. \n",
    "      #You need to set the steps_per_epoch argument of FIT METHOD to n_samples / batch_size, n_samples is the total number of training data you have (ex:1000 in your case).\n",
    "      #This way in each epoch, each training sample is augmented only one time and therefore 1000 transformed images will be generated in each epoch.\n",
    "      # These augmented images are not stored in the memory, they are generated on the fly while training and lost after training\n",
    "    \n",
    "      #steps_per_epoch: The steps_per_epoch argument must specify the number of batches of samples comprising one epoch. \n",
    "      #For example, if your original dataset has 10,000 images and your batch size is 32, then a reasonable value for steps_per_epoch \n",
    "      #when fitting a model on the augmented data might be ceil(10,000/32), or 313 batches. \n",
    "      #Debido a que los datos se est치n generando infinitamente, el c칩digo necesita saber cu치ntas muestras extraer del generador antes de declarar una 칠poca\n",
    "      steps_per_epoch=math.ceil(training_samples/batch_size_training_generator),\n",
    "      epochs=15,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=math.ceil(validation_samples/batch_size_validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbFtGAkz0epS"
   },
   "outputs": [],
   "source": [
    "#Guardamos el modelo\n",
    "model.save('model_weedcrops.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "dhV3hR6b000a",
    "outputId": "337d9a59-1c38-4215-b471-388c899742a3"
   },
   "outputs": [],
   "source": [
    "#An치lisis de resultado de accy y loss de train y validation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h75RwjD4e5hW"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCkNuxP0e8PH"
   },
   "outputs": [],
   "source": [
    "test_samples =3067\n",
    "batch_size_test=31\n",
    "\n",
    "score= model.evaluate_generator(test_generator, steps = math.ceil(test_samples/batch_size_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9WNRLfcbiCB6",
    "outputId": "91521045-0875-45b4-d61e-5f95ad8d0985"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uB9E-JnYL0FU"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PYPn2eUqTpl-",
    "outputId": "1e3ea017-4db6-4b5d-a211-94abdd450636"
   },
   "outputs": [],
   "source": [
    "#When predicting, you have to respect this shape even if you have only one image. Your input should be of shape: [1, image_width, image_height, number_of_channels]\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "def load(filename):\n",
    "   np_image = Image.open(filename) #Open the image\n",
    "   np_image = np.array(np_image).astype('float32')/255 #Creates a numpy array as float and divides by 255.\n",
    "   np_image = transform.resize(np_image, (150, 150, 3)) #Resize a 150x150 con 3 channels\n",
    "   # La reescalamos a (1, 150, 150, 3)\n",
    "   #Since you trained your model on mini-batches, your input is a tensor of shape [batch_size, image_width, image_height, number_of_channels].\n",
    "   np_image = np.expand_dims(np_image, axis=0) #Insert a new axis that will appear at the axis position in the expanded array shape.\n",
    "   return np_image\n",
    "\n",
    "#Mostramos los labels\n",
    "label_map = (test_generator.class_indices)\n",
    "print (label_map)\n",
    "\n",
    "#Tomamos una imagen del test. EL label correcto es weed\n",
    "image_to_predict = load('D:/Project 2/dataset/broadleaf/1.tif')\n",
    "result = model.predict(image_to_predict)\n",
    "result= np.around(result,decimals=3)\n",
    "result=result*100\n",
    "print (result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "xOiy1C-byTHh",
    "outputId": "4a7a84fb-4d2a-417d-ffab-ab72f6530a22"
   },
   "outputs": [],
   "source": [
    "#Transformar la imagen de (1, 150, 150, 3) a (150, 150, 3) y mostrarla\n",
    "image_to_predict= np.squeeze(image_to_predict,axis=0)\n",
    "image_to_predict.shape\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(image_to_predict, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-kaROzVQt00A"
   ],
   "name": "Malezas_Colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
